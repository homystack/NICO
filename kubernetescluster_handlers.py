#!/usr/bin/env python3

import kopf
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Optional

from clients import (
    get_machine,
    list_machines,
    create_secret,
    delete_secret,
    get_secret_data,
    update_cluster_status,
    create_nixos_configuration,
    delete_nixos_configuration,
    get_nixos_configuration,
)

logger = logging.getLogger(__name__)


async def select_machines_for_cluster(
    cluster_spec: dict, namespace: str, role: str
) -> List[str]:
    """
    Select machines for cluster role (controlPlane or dataPlane)
    Priority: explicit machines list > machineSelector + count
    """
    role_spec = cluster_spec.get(role, {})
    
    # If explicit machines list is provided, use it
    if role_spec.get("machines"):
        return role_spec["machines"]
    
    # Otherwise use machineSelector and count
    machine_selector = role_spec.get("machineSelector", {})
    count = role_spec.get("count", 0)
    
    if not machine_selector or count == 0:
        return []
    
    # Get all machines in namespace
    machines = list_machines(namespace)
    
    # Filter by selector and hasConfiguration: false
    selected_machines = []
    for machine in machines:
        # Check selector match
        match = True
        machine_labels = machine.get("metadata", {}).get("labels", {})
        for key, value in machine_selector.get("matchLabels", {}).items():
            if machine_labels.get(key) != value:
                match = False
                break
        
        # Check machine is available (no configuration applied)
        if match and not machine.get("status", {}).get("hasConfiguration", True):
            selected_machines.append(machine["metadata"]["name"])
    
    # Return up to count machines
    return selected_machines[:count]


def generate_cluster_config(
    cluster_name: str, 
    control_plane_nodes: List[str], 
    worker_nodes: List[str],
    machines_info: Dict[str, Dict]
) -> str:
    """Generate cluster.nix configuration with all nodes including IP addresses"""
    
    # Generate control plane nodes with IPs
    control_plane_with_ips = []
    for node_name in control_plane_nodes:
        machine_info = machines_info.get(node_name, {})
        ip_address = machine_info.get("ipAddress") or machine_info.get("hostname", "unknown")
        control_plane_with_ips.append({
            "name": node_name,
            "ip": ip_address
        })
    
    # Generate worker nodes with IPs
    workers_with_ips = []
    for node_name in worker_nodes:
        machine_info = machines_info.get(node_name, {})
        ip_address = machine_info.get("ipAddress") or machine_info.get("hostname", "unknown")
        workers_with_ips.append({
            "name": node_name,
            "ip": ip_address
        })
    
    # Format as Nix expression
    control_plane_nix = "[\n" + "\n".join(
        f'    {{ name = "{node["name"]}"; ip = "{node["ip"]}"; }}'
        for node in control_plane_with_ips
    ) + "\n  ]"
    
    workers_nix = "[\n" + "\n".join(
        f'    {{ name = "{node["name"]}"; ip = "{node["ip"]}"; }}'
        for node in workers_with_ips
    ) + "\n  ]"
    
    return f"""
{{ config, pkgs, ... }}:
{{
  # Cluster configuration generated by NICO
  # Includes IP addresses for HAProxy configuration at build time
  cluster = {{
    name = "{cluster_name}";
    controlPlane = {control_plane_nix};
    workers = {workers_nix};
  }};
}}
"""


async def create_join_token_secret(
    cluster_name: str, namespace: str
) -> str:
    """Create a secret with join token for cluster nodes"""
    # In a real implementation, this would generate actual tokens
    # For now, we'll create a placeholder
    token_content = f"join-token-for-{cluster_name}"
    
    secret_name = f"{cluster_name}-join-token"
    
    await create_secret(
        secret_name,
        namespace,
        {
            "token": token_content,
        },
    )
    
    return secret_name


async def create_nixos_configuration_for_machine(
    cluster_name: str,
    machine_name: str,
    role: str,
    cluster_spec: dict,
    namespace: str,
    join_token_secret: str,
    control_plane_nodes: List[str],
    worker_nodes: List[str],
    machines_info: Dict[str, Dict],
) -> str:
    """Create NixosConfiguration for a cluster machine"""
    
    # Get machine to check SSH key and determine if fullInstall is needed
    machine = get_machine(machine_name, namespace)
    
    # Check if machine has the full installation annotation
    annotations = machine.get("metadata", {}).get("annotations", {})
    has_full_install = annotations.get("nico.homystack.com/fullInstallationApplied")
    
    # Determine SSH key secret name from machine spec
    ssh_key_secret = None
    if machine["spec"].get("sshKeySecretRef"):
        ssh_key_secret = machine["spec"]["sshKeySecretRef"]["name"]
    
    # Build additional files
    additional_files = [
        {
            "path": "cluster.nix",
            "valueType": "Inline",
            "inline": generate_cluster_config(cluster_name, control_plane_nodes, worker_nodes, machines_info),
        },
        {
            "path": "join-token",
            "valueType": "SecretRef", 
            "secretRef": {"name": join_token_secret},
        },
    ]
    
    # Add SSH key if available
    if ssh_key_secret:
        additional_files.append({
            "path": "machine-ssh-key",
            "valueType": "SecretRef",
            "secretRef": {"name": ssh_key_secret},
        })
    
    # Create NixosConfiguration
    config_name = f"{cluster_name}-{machine_name}"
    
    nixos_config_spec = {
        "gitRepo": cluster_spec["gitRepo"],
        "flake": f"#{machine_name}",  # Use machine name as flake reference
        "onRemoveFlake": "#standBy",  # Default cleanup flake
        "configurationSubdir": cluster_spec.get("configurationSubdir", ""),
        "machineRef": {"name": machine_name},
        "fullInstall": not has_full_install,  # Full install only if not already done
        "additionalFiles": additional_files,
    }
    
    # Add credentials if specified
    if cluster_spec.get("credentialsRef"):
        nixos_config_spec["credentialsRef"] = cluster_spec["credentialsRef"]
    
    await create_nixos_configuration(config_name, namespace, nixos_config_spec)
    
    return config_name


async def reconcile_kubernetes_cluster(body, spec, name, namespace, **kwargs):
    """Main reconciliation point for KubernetesCluster"""
    logger.info(f"Reconciling KubernetesCluster: {name}")
    
    try:
        deletion_timestamp = body.get("metadata", {}).get("deletionTimestamp")
        
        if deletion_timestamp:
            # Handle cluster deletion
            await handle_cluster_deletion(name, namespace, body)
            return
        
        # Select machines for cluster roles
        control_plane_nodes = await select_machines_for_cluster(
            spec, namespace, "controlPlane"
        )
        worker_nodes = await select_machines_for_cluster(
            spec, namespace, "dataPlane"
        )
        
        if not control_plane_nodes:
            raise kopf.TemporaryError(
                "No available control plane machines found", delay=60
            )
        
        # Create join token secret
        join_token_secret = await create_join_token_secret(name, namespace)
        
        # Collect machine information for cluster.nix
        machines_info = {}
        all_machine_names = control_plane_nodes + worker_nodes
        for machine_name in all_machine_names:
            machine = get_machine(machine_name, namespace)
            machines_info[machine_name] = {
                "hostname": machine["spec"].get("hostname", ""),
                "ipAddress": machine["spec"].get("ipAddress", ""),
            }
        
        # Track created configurations
        applied_configs = {}
        
        # Create configurations for control plane nodes
        for machine_name in control_plane_nodes:
            config_name = await create_nixos_configuration_for_machine(
                name, machine_name, "control-plane", spec, namespace,
                join_token_secret, control_plane_nodes, worker_nodes, machines_info
            )
            applied_configs[machine_name] = config_name
        
        # Create configurations for worker nodes  
        for machine_name in worker_nodes:
            config_name = await create_nixos_configuration_for_machine(
                name, machine_name, "worker", spec, namespace,
                join_token_secret, control_plane_nodes, worker_nodes, machines_info
            )
            applied_configs[machine_name] = config_name
        
        # Update cluster status
        await update_cluster_status(
            name,
            namespace,
            {
                "phase": "Provisioning",
                "controlPlaneReady": f"0/{len(control_plane_nodes)}",
                "dataPlaneReady": f"0/{len(worker_nodes)}",
                "kubeconfigSecret": f"{name}-kubeconfig",  # Will be created later
                "appliedMachines": applied_configs,
                "conditions": [
                    {
                        "type": "Provisioning",
                        "status": "True",
                        "lastTransitionTime": datetime.utcnow().isoformat() + "Z",
                        "reason": "ConfigurationsCreated",
                        "message": f"Created configurations for {len(applied_configs)} machines",
                    }
                ],
            },
        )
        
        logger.info(f"Successfully started provisioning cluster {name}")
        
    except Exception as e:
        logger.error(f"Failed to reconcile KubernetesCluster {name}: {e}")
        raise kopf.TemporaryError(f"Cluster reconciliation failed: {e}", delay=60)


async def handle_cluster_deletion(cluster_name: str, namespace: str, body: dict):
    """Handle deletion of KubernetesCluster"""
    logger.info(f"Handling deletion of cluster: {cluster_name}")
    
    # Get applied configurations from status
    status = body.get("status", {})
    applied_configs = status.get("appliedMachines", {})
    
    # Delete all NixosConfigurations
    for machine_name, config_name in applied_configs.items():
        try:
            await delete_nixos_configuration(config_name, namespace)
            logger.info(f"Deleted NixosConfiguration: {config_name}")
        except Exception as e:
            logger.warning(f"Failed to delete NixosConfiguration {config_name}: {e}")
    
    # Delete join token secret
    try:
        join_token_secret = f"{cluster_name}-join-token"
        await delete_secret(join_token_secret, namespace)
        logger.info(f"Deleted join token secret: {join_token_secret}")
    except Exception as e:
        logger.warning(f"Failed to delete join token secret: {e}")
    
    # Delete kubeconfig secret (if exists)
    try:
        kubeconfig_secret = f"{cluster_name}-kubeconfig"
        await delete_secret(kubeconfig_secret, namespace)
        logger.info(f"Deleted kubeconfig secret: {kubeconfig_secret}")
    except Exception as e:
        logger.warning(f"Failed to delete kubeconfig secret: {e}")
    
    logger.info(f"Cluster {cluster_name} deletion completed")


@kopf.timer("nico.homystack.com", "v1alpha1", "kubernetesclusters", interval=30.0)
async def monitor_cluster_status(body, spec, name, namespace, **kwargs):
    """Monitor and update cluster status based on machine states"""
    logger.debug(f"Monitoring cluster status: {name}")
    
    try:
        status = body.get("status", {})
        applied_configs = status.get("appliedMachines", {})
        
        if not applied_configs:
            return
        
        # Count ready machines per role
        control_plane_ready = 0
        data_plane_ready = 0
        total_control_plane = 0
        total_data_plane = 0
        
        for machine_name, config_name in applied_configs.items():
            # Get machine to determine role
            machine = get_machine(machine_name, namespace)
            machine_labels = machine.get("metadata", {}).get("labels", {})
            
            # Determine role from labels or position in applied_configs
            # This is a simplified approach - in practice we'd track roles better
            is_control_plane = "control-plane" in machine_labels.get("role", "").lower()
            
            # Get configuration status
            config = get_nixos_configuration(config_name, namespace)
            config_status = config.get("status", {})
            
            # Check if configuration is applied and machine is ready
            if (config_status.get("appliedCommit") and 
                machine.get("status", {}).get("hasConfiguration")):
                
                if is_control_plane:
                    control_plane_ready += 1
                else:
                    data_plane_ready += 1
            
            # Count totals
            if is_control_plane:
                total_control_plane += 1
            else:
                total_data_plane += 1
        
        # Determine cluster phase
        phase = "Provisioning"
        if control_plane_ready == total_control_plane and control_plane_ready > 0:
            phase = "Ready"
        
        # Update cluster status
        await update_cluster_status(
            name,
            namespace,
            {
                "phase": phase,
                "controlPlaneReady": f"{control_plane_ready}/{total_control_plane}",
                "dataPlaneReady": f"{data_plane_ready}/{total_data_plane}",
                "conditions": [
                    {
                        "type": "Ready",
                        "status": "True" if phase == "Ready" else "False",
                        "lastTransitionTime": datetime.utcnow().isoformat() + "Z",
                        "reason": "AllNodesReady" if phase == "Ready" else "Provisioning",
                        "message": f"Control plane: {control_plane_ready}/{total_control_plane}, Workers: {data_plane_ready}/{total_data_plane}",
                    }
                ],
            },
        )
        
    except Exception as e:
        logger.error(f"Failed to monitor cluster {name} status: {e}")
