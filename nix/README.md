# NixOS Flake for NICO Operator

This flake provides automatic Kubernetes cluster provisioning with k3s and keepalived through NICO operator.

## File Structure

```
nix/
├── flake.nix              # Main flake file
├── cluster.nix            # Cluster configuration (generated by NICO)
├── machine-ssh-private-key # SSH private key (injected by NICO)
├── join-token             # K3s join token (injected by NICO)
├── README.md              # This documentation
└── nodes/
    ├── generator.nix      # Configuration generator for all nodes
    ├── common.nix         # Common configuration for all nodes
    ├── control-plane.nix  # Control plane config with keepalived + k3s server
    ├── worker.nix         # Worker config with k3s agent
    ├── minimal.nix        # Minimal configuration for cleanup
    └── disko-config.nix   # Disk layout (10GB system + LVM)
```

## Files Injected by NICO

NICO automatically creates and injects the following files into `configurationSubdir` root:

### 1. `cluster.nix` (required)

Generated by operator based on `KubernetesCluster` resource:

```nix
{
  name = "production-cluster";
  vip = "192.168.1.100";  # VIP for HA control plane

  controlPlane = [
    { name = "cp-01"; ip = "192.168.1.101"; }
    { name = "cp-02"; ip = "192.168.1.102"; }
    { name = "cp-03"; ip = "192.168.1.103"; }
  ];

  workers = [
    { name = "worker-01"; ip = "192.168.1.201"; }
    { name = "worker-02"; ip = "192.168.1.202"; }
  ];
}
```

### 2. `machine-ssh-private-key` (required)

SSH private key for inter-node communication. NICO injects it from Secret specified in `Machine.spec.sshKeySecretRef`.

**Format**: ED25519 private key without passphrase

**Usage**:
- Public key is automatically generated during build
- Public key is added to `authorized_keys` on all nodes
- Private key is copied to `/root/.ssh/id_ed25519` for inter-node SSH

### 3. `join-token` (required)

Token for joining nodes to k3s cluster. NICO generates this from the cluster-wide join token secret.

**Usage**:
- Copied to `/var/lib/rancher/k3s/server/token` on control plane
- Copied to `/var/lib/rancher/k3s/agent/token` on workers

## How It Works

### Control Plane Nodes

#### First Master (nodeIndex == 0)
1. **Keepalived**: Starts in MASTER mode with priority=150
2. **K3s**: Initializes cluster with `--cluster-init`
3. **API Server**: Available on VIP and own IP
4. **Etcd**: Starts embedded etcd

#### Other Masters (nodeIndex > 0)
1. **Keepalived**: Start in BACKUP mode with priority=149, 148, ...
2. **K3s**: Wait for VIP availability, then join the cluster
3. **Etcd**: Join the embedded etcd cluster

### Worker Nodes

1. Wait for control plane VIP availability
2. Join cluster as agents via VIP
3. Don't run keepalived

### Keepalived VIP

- **VIP**: Assigned from `cluster.nix`
- **VRRP ID**: 51 (fixed)
- **Interface**: eth0 (configurable)
- **Authentication**: PASS with password generated from cluster name hash

## Disk Layout (disko-config.nix)

```
/dev/sda:
├── 1MB    - BIOS boot (EF02)
├── 512MB  - EFI System (EF00) -> /boot
├── 10GB   - Root (ext4) -> /
└── Rest   - LVM PV -> vg0
              └── thinpool (100% FREE)
```

**Features**:
- System uses exactly 10GB
- Remaining space goes to LVM for flexible management
- Swap is disabled (Kubernetes requirement)

## Generated Configurations

Based on `cluster.nix`, the flake automatically generates:

```nix
nixosConfigurations = {
  cp-01 = /* control plane node 0 */;
  cp-02 = /* control plane node 1 */;
  cp-03 = /* control plane node 2 */;
  worker-01 = /* worker node */;
  worker-02 = /* worker node */;
  minimal = /* cleanup configuration */;
};
```

Each configuration uses:
- `#<machine-name>` as flake output
- Node-specific IP from `cluster.nix`
- Role-specific modules (control-plane.nix or worker.nix)

## Usage with NICO

### 1. Create KubernetesCluster Resource

```yaml
apiVersion: nico.homystack.com/v1alpha1
kind: KubernetesCluster
metadata:
  name: prod-cluster
  namespace: default
spec:
  gitRepo: "https://github.com/your-org/nico-configs.git"
  configurationSubdir: "nix"  # This directory

  controlPlane:
    machineSelector:
      matchLabels:
        role: control-plane
    count: 3

  dataPlane:
    machineSelector:
      matchLabels:
        role: worker
    count: 5
```

### 2. NICO Automatically:

1. Generates `cluster.nix` with all node information
2. Injects `machine-ssh-private-key` from Machine secret
3. Injects `join-token` for k3s
4. Creates `NixosConfiguration` for each node:
   - For cp-01: `flake: "#cp-01"`
   - For cp-02: `flake: "#cp-02"`
   - For worker-01: `flake: "#worker-01"`

### 3. Flake Generates Configurations

```bash
# List all configurations
nix flake show

# Example outputs:
# nixosConfigurations.cp-01
# nixosConfigurations.cp-02
# nixosConfigurations.cp-03
# nixosConfigurations.worker-01
# nixosConfigurations.worker-02
# nixosConfigurations.minimal  # for cleanup
```

## Cleanup (Cluster Deletion)

When deleting a `KubernetesCluster`:

1. NICO applies `#minimal` configuration to all nodes
2. `minimal.nix` stops all k3s services
3. Removes k3s data from `/var/lib/rancher/k3s`
4. Machines become available for reuse

## Machine Requirements

Each machine must have:

```yaml
apiVersion: nio.homystack.com/v1alpha1
kind: Machine
metadata:
  name: cp-01
  labels:
    role: control-plane  # or worker
spec:
  hostname: cp-01.local
  ipAddress: 192.168.1.101
  sshUser: root
  sshKeySecretRef:
    name: machine-ssh-key  # ED25519 private key
```

## Network Requirements

### Firewall (automatically configured)

**Control Plane**:
- TCP: 22 (SSH), 6443 (API), 2379-2380 (etcd), 10250-10259 (kubelet, scheduler, controller)
- UDP: 8472 (Flannel VXLAN)

**Workers**:
- TCP: 22 (SSH), 10250 (kubelet), 30000-32767 (NodePort)
- UDP: 8472 (Flannel VXLAN)

### Network

- All nodes must be in the same L2 network for keepalived
- VIP must be from the same subnet as node IPs
- Default gateway and DNS are configured in `generator.nix` (defaults: 192.168.1.1 and 8.8.8.8)

## Accessing the Cluster

After deployment, kubeconfig is available:

```bash
# On first control plane node
ssh root@192.168.1.101
cat /etc/rancher/k3s/k3s.yaml

# Or via VIP
ssh root@192.168.1.100
kubectl get nodes
```

## Monitoring

### Check Keepalived

```bash
# On control plane nodes
systemctl status keepalived
ip addr show eth0  # Should show VIP on MASTER
```

### Check K3s

```bash
# K3s logs
journalctl -u k3s -f

# Node status
kubectl get nodes -o wide

# Check etcd health
kubectl get --raw /healthz/etcd
```

## Troubleshooting

### VIP Not Assigned

1. Check all control plane nodes are in same L2 network
2. Check keepalived logs: `journalctl -u keepalived`
3. Ensure interface name is correct (eth0)

### Nodes Not Joining Cluster

1. Check VIP availability: `ping 192.168.1.100`
2. Check token: `cat /var/lib/rancher/k3s/server/token`
3. Check k3s logs: `journalctl -u k3s -f`
4. Check firewall: `iptables -L -n`

### SSH Keys Not Working

1. Ensure `machine-ssh-private-key` is present in directory
2. Check key format: `ssh-keygen -y -f machine-ssh-private-key`
3. Check authorized_keys: `cat /root/.ssh/authorized_keys`

## Customization

### Change VIP Subnet

In `generator.nix` change:

```nix
networking.defaultGateway = {
  address = "YOUR_GATEWAY";
  interface = "eth0";
};
```

### Change Interface for Keepalived

In `control-plane.nix` change:

```nix
services.keepalived.vrrpInstances.k8s_vip.interface = "ens18";
```

### Add Additional Disks to LVM

Edit `disko-config.nix` to add more disks to the LVM volume group.

## Versions

- **NixOS**: 24.05
- **K3s**: Latest from nixpkgs
- **Keepalived**: Latest from nixpkgs
- **Disko**: Latest from flake inputs
